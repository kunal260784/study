{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88655023",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "from langchain_tavily import TavilySearch\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "844ed6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tavily_tool = TavilySearch(\n",
    "    maxResults = 5,\n",
    "  topic = \"general\"   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7bbfaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikitool = WikipediaQueryRun(\n",
    "    api_wrapper=WikipediaAPIWrapper(\n",
    "        top_k_results=5,\n",
    "        doc_content_chars_max=2000\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b30cc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = init_chat_model(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    model_provider= \"openai\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28d57d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "llmagent = create_agent(\n",
    "    model=llm,\n",
    "    tools=[wikitool, tavily_tool]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99b5a2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\RAG\\learningRAG\\.venv\\Lib\\site-packages\\wikipedia\\wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file c:\\RAG\\learningRAG\\.venv\\Lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is LLM?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  wikipedia (call_BTPAo6s998M83nyyTIBO0WyS)\n",
      " Call ID: call_BTPAo6s998M83nyyTIBO0WyS\n",
      "  Args:\n",
      "    query: LLM\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: wikipedia\n",
      "\n",
      "Page: Large language model\n",
      "Summary: A large language model (LLM) is a language model trained with self-supervised machine learning on a vast amount of text, designed for natural language processing tasks, especially language generation. The largest and most capable LLMs are generative pre-trained transformers (GPTs) and provide the core capabilities of modern chatbots. LLMs can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained on.\n",
      "They consist of billions to trillions of parameters and operate as general-purpose sequence models, generating, summarizing, translating, and reasoning over text. LLMs represent a significant new technology in their ability to generalize across tasks with minimal task-specific supervision, enabling capabilities like conversational agents, code generation, knowledge retrieval, and automated reasoning that previously required bespoke systems.\n",
      "LLMs evolved from earlier statistical and recurrent neural network approaches to language modeling. The transformer architecture, introduced in 2017, replaced recurrence with self-attention, allowing efficient parallelization, longer context handling, and scalable training on unprecedented data volumes. This innovation enabled models like GPT, BERT, and their successors, which demonstrated emergent behaviors at scale, such as few-shot learning and compositional reasoning.\n",
      "Reinforcement learning, particularly policy gradient algorithms, has been adapted to fine-tune LLMs for desired behaviors beyond raw next-token prediction. Reinforcement learning from human feedback (RLHF) applies these methods to optimize a policy, the LLM's output distribution, against reward signals derived from human or automated preference judgments. This has been critical for aligning model outputs with user expectat\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "LLM stands for Large Language Model. It is a language model trained using self-supervised machine learning on a vast amount of text, primarily designed for natural language processing tasks, especially language generation. The most advanced LLMs are often based on architectures known as generative pre-trained transformers (GPTs).\n",
      "\n",
      "Key features of LLMs include:\n",
      "\n",
      "1. **Ability to Generate and Understand Text**: LLMs can generate, summarize, translate, and reason over text, making them versatile tools for various language-related tasks.\n",
      "\n",
      "2. **Large Parameter Count**: These models contain billions to trillions of parameters, enabling them to understand and predict the nuances of human language.\n",
      "\n",
      "3. **Training on Massive Datasets**: LLMs are trained on extensive datasets, allowing them to generalize across different tasks with minimal task-specific supervision.\n",
      "\n",
      "4. **Transformer Architecture**: They leverage the transformer architecture, which uses self-attention mechanisms rather than recurrence, allowing for efficient processing of long contexts.\n",
      "\n",
      "5. **Fine-tuning and Reinforcement Learning**: LLMs can be fine-tuned for specific applications, and reinforcement learning techniques, like reinforcement learning from human feedback (RLHF), are often used to align their outputs with user expectations.\n",
      "\n",
      "Overall, LLMs represent a significant technological advancement in the field of artificial intelligence, enabling capabilities that were previously difficult to achieve with customized systems.\n"
     ]
    }
   ],
   "source": [
    "result = llmagent.invoke({\"messages\": \"What is LLM?\"})\n",
    "for item in result['messages']:\n",
    "    item.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
